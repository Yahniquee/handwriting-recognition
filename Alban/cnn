from scipy import io as sio
import tensorflow as tf
import numpy as np
import keras
from keras.models import Sequential,Input,Model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.layers.normalization import BatchNormalization
from keras.layers.advanced_activations import LeakyReLU

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split


#step 1 loading and preprocessing EMNIST dataset 

 

  
import numpy as np
import sys

from keras.datasets import mnist
import tensorflow as tf
import tensorflow_datasets as tfds
import pickle
import gzip
import matplotlib.pyplot as plt

# load train and test dataset
def load_mnist_online():
    """Do not use this one"""
    # load dataset
    (trainX, trainY), (testX, testY) = mnist.load_data()
    # reshape dataset to have a single channel
    # trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
    # testX = testX.reshape((testX.shape[0], 28, 28, 1))
    # one hot encode target values
    # trainY = to_categorical(trainY)
    # testY = to_categorical(testY)
    return trainX, trainY, testX, testY

def load_mnist_offline():
    """
    Taken from Nielsen, adapted for Python 3.
    Return the MNIST data as a tuple containing the training data,
    the validation data, and the test data.
    The ``training_data`` is returned as a tuple with two entries.
    The first entry contains the actual training images.  This is a
    numpy ndarray with 50,000 entries.  Each entry is, in turn, a
    numpy ndarray with 784 values, representing the 28 * 28 = 784
    pixels in a single MNIST image.
    The second entry in the ``training_data`` tuple is a numpy ndarray
    containing 50,000 entries.  Those entries are just the digit
    values (0...9) for the corresponding images contained in the first
    entry of the tuple.
    The ``validation_data`` and ``test_data`` are similar, except
    each contains only 10,000 images.
    This is a nice data format, but for use in neural networks it's
    helpful to modify the format of the ``training_data`` a little.
    That's done in the wrapper function ``load_data_wrapper()``, see
    below.
    training_data, validation_data, test_data = cPickle.load(f)
    """
    f = gzip.open('../data/MNIST/mnist.pkl.gz', 'rb')
    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')
    f.close()
    return (training_data, validation_data, test_data)

def load_mnist():
    """ Loads the MNIST dataset via tensorflow_datasets.load(). At first execution downloads the database to a local
    directory (see documentation for tensorflow_datasets.load()), after that grabs database from this local directory.
    Returns training_data (60.000) and testing_data (10.000) each as a tuple of an input array 60.000x28x28 resp.
    10.000x28x28 with values between 0 and 255 and a result array of size 60.000 resp. 10.000 containing the associated
    label between 0 and 61 representing number, small letters and capital letters"""

    print('Loading MNIST database, this might take a while...')
    data = tfds.as_numpy(tfds.load(
        'mnist',
        batch_size=-1,
        as_supervised=True,
    ))

    training_data = list(data['train'])
    testing_data = list(data['test'])
    training_data[0] = training_data[0][:, :, :, 0]
    testing_data[0] = testing_data[0][:, :, :, 0]
    training_data = tuple(training_data)
    testing_data = tuple(testing_data)

    return (training_data, testing_data)

def load_emnist():
    """ Loads the EMNIST dataset via tensorflow_datasets.load(). At first execution downloads the database to a local
    directory (see documentation for tensorflow_datasets.load()), after that grabs database from this local directory.
    Returns training_data (697932) and testing_data (116323) each as a tuple of an input array 697932x28x28 resp.
    116323x28x28 with values between 0 and 255 and a result array of size 60.000 resp. 10.000 containing the associated
    labels between 0
    Warning: these tuples are very large and take up a lot of RAM"""
    print('Loading EMNIST database, this might take a while...')
    data = tfds.as_numpy(tfds.load(
        'emnist',
        batch_size=-1,
        as_supervised=True,
    ))

    training_data = list(data['train'])
    testing_data = list(data['test'])
    training_data[0] = tf.image.rot90(training_data[0], k=3)
    testing_data[0] = tf.image.rot90(testing_data[0], k=3)
    training_data[0] = tf.image.flip_left_right(training_data[0])
    testing_data[0] = tf.image.flip_left_right(testing_data[0])
    testing_data[0] = testing_data[0].numpy()
    training_data[0] = training_data[0].numpy()
    training_data[0] = training_data[0][:, :, :, 0]
    testing_data[0] = testing_data[0][:, :, :, 0]
    training_data = tuple(training_data)
    testing_data = tuple(testing_data)

    return (training_data, testing_data)


# The following is only relevant for neuralnetnp by Nielsen.
def vectorized_result(j):
    """ From Nielsen, used in load_data_wrapper()
    Return a 10-dimensional unit vector with a 1.0 in the jth
    position and zeroes elsewhere.  This is used to convert a digit
    (0...9) into a corresponding desired output from the neural
    network."""
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e

def load_data_wrapper():
    """ From Nielsen, adapted for Python 3.
    Return a tuple containing ``(training_data, validation_data,
    test_data)``. Based on ``load_data``, but the format is more
    convenient for use in our implementation of neural networks.
    In particular, ``training_data`` is a list containing 50,000
    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray
    containing the input image.  ``y`` is a 10-dimensional
    numpy.ndarray representing the unit vector corresponding to the
    correct digit for ``x``.
    ``validation_data`` and ``test_data`` are lists containing 10,000
    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional
    numpy.ndarry containing the input image, and ``y`` is the
    corresponding classification, i.e., the digit values (integers)
    corresponding to ``x``.
    Obviously, this means we're using slightly different formats for
    the training data and the validation / test data.  These formats
    turn out to be the most convenient for use in our neural network
    code."""

    tr_d, va_d, te_d = load_mnist_offline()
    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]
    training_results = [vectorized_result(y) for y in tr_d[1]]
    training_data = list(zip(training_inputs, training_results))
    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]
    validation_data = list(zip(validation_inputs, va_d[1]))
    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]
    test_data = list(zip(test_inputs, te_d[1]))
    return (training_data, validation_data, test_data)









'''mat = sio.loadmat('emnist-letters.mat')
data = mat['dataset']



# this type of sample did not work as the other way of splitting so I pass for now 
x_train, x_test, y_train, y_test = train_test_split(data['train'][0,0]['images'][0,0],
                                                    data['train'][0,0]['images'][0,0],
                                                    test_size=0.2)
                                                    

#                                                              random_state=42

#define what the training and testing data are

x_train = data['train'][0,0]['images'][0,0] 
y_train = data['train'][0,0]['labels'][0,0]
x_test = data['test'][0,0]['images'][0,0]
y_test = data['test'][0,0]['labels'][0,0]

#I did not use cross-validation by now since it was not that inportant in this case
#_train = data['test'][0,0]['labels'][0,0]
#using cros validation 
#al_start = x_train.shape[0] - x_test.shape[0]
#_val =x_train[val_start:x_train.shape[0]]
#_val = y_train[val_start:y_train.shape[0]]
#_train = x_train[0:val_start]
#_train = y_train[0:val_start]'''


#reshape the arrays into image

#x_train = x_train.reshape( (x_train.shape[0], 28, 28), order='F')


    
    
    
  
    
#_train.shape, y_train.shape

#_test.shape, y_test.shape


#_train = x_train[..., tf.newaxis]
#_test = x_test[..., tf.newaxis]

#_train.shape

#_test.shape

#p.min(x_train), np.max(x_train)

#x_train = x_train / 255.
#x_test=x_test/255

#p.min(x_train), np.max(x_train)
#x_train = x_train.reshape( (x_train.shape[0], 28, 28), order='F')

#y_train = y_train.reshape( (y_train.shape[0], 28, 28), order='F')'''
#x_test = x_test.reshape( (x_test.shape[0], 28, 28), order='F')


#number of unique classes
'''classes = np.unique(y_train)
nClasses = len(classes)
print('Total number of outputs : ', nClasses)
print('Output classes : ', classes)


#reshaping the training and testing data


print('train shape', x_train.shape)
print('test shape', x_test.shape)'''






import tensorflow as tf
import numpy as np
import keras
from keras.models import Sequential,Input,Model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.layers.normalization import BatchNormalization
from keras.layers.advanced_activations import LeakyReLU

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

training_data, test_data = load_emnist()
(x_train, y_train) = training_data
(x_test, y_test) = test_data





x_train = x_train.reshape( (x_train.shape[0], 28, 28), order='F')
x_test = x_test.reshape( (x_test.shape[0], 28, 28), order='F')

#number of unique classes
classes = np.unique(y_train)
nClasses = len(classes)
print('Total number of outputs : ', nClasses)
print('Output classes : ', classes)


#reshaping the training and testing data


print('train shape', x_train.shape)
print('test shape', x_test.shape)



import matplotlib.pyplot as plt
plt.subplot(121)
plt.imshow(x_train[0,:,:], cmap='gray')
plt.title("Ground Truth : {}".format(y_train[0]))

# Display the first image in testing data
plt.subplot(122)
plt.imshow(x_test[0,:], cmap='gray')
plt.title("Ground Truth : {}".format(y_test[0]))


#reshaping the training and testing data
x_train = x_train.reshape(-1, 28,28, 1)
x_test = x_test.reshape(-1, 28,28, 1)
x_train.shape, x_test.shape


x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train = x_train / 255.
x_test = x_test / 255.

from keras.utils import to_categorical
y_train_one_hot = to_categorical(y_train)
y_test_one_hot = to_categorical(y_test)


batch_size = 64
epochs = 3
num_classes = 62


#built the model

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D((2, 2),padding='same'))
model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))
model.add(LeakyReLU(alpha=0.1))                  
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Flatten())
model.add(Dense(128, activation='linear'))
model.add(LeakyReLU(alpha=0.1))                  
model.add(Dense(num_classes, activation='softmax'))
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])


model.summary()

'''Let's visualize the layers that you created in the above step by using the summary function. 
This will show some parameters (weights and biases) in each layer and also the total parameters in your model'''

#train the model/fiting 

model_train = model.fit(x_train, y_train_one_hot, batch_size=batch_size,epochs=epochs, verbose =1)
